{"componentChunkName":"component---src-templates-markdown-remark-js","path":"/community-handbook/sre/prometheus_alerting_consistency.md","result":{"data":{"site":{"siteMetadata":{"title":"Operate First"}},"markdownRemark":{"id":"9f2d9ca9-ce65-5a54-9655-b03d5520f68b","html":"<h1 id=\"prometheus-alerting-consistency\" style=\"position:relative;\"><a href=\"#prometheus-alerting-consistency\" aria-label=\"prometheus alerting consistency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Prometheus Alerting Consistency</h1>\n<h2 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Summary</h2>\n<p>This guide is originally based on the <a href=\"https://github.com/openshift/enhancements/blob/master/enhancements/monitoring/alerting-consistency.md\">Alerting Consistency</a> proposal for OpenShift alerts.</p>\n<p>Clear and actionable alerts are a key component of a smooth operational experience. If an alert doesn’t have a proven documented procedure to follow when it fires, it can lead to an increase in mean time to repair (MTTR).\nEnsuring you have clear and concise guidelines for engineers and SRE creating new alerts will result in a better experience for end users.</p>\n<h2 id=\"recommended-reading\" style=\"position:relative;\"><a href=\"#recommended-reading\" aria-label=\"recommended reading permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Recommended Reading</h2>\n<p>A list of references on good alerting practices:</p>\n<ul class=\"pf-c-list\">\n<li><a href=\"https://sre.google/sre-book/monitoring-distributed-systems/\">Google SRE Book - Monitoring Distributed Systems</a></li>\n<li><a href=\"https://prometheus.io/docs/practices/alerting/\">Prometheus Alerting Documentation</a></li>\n<li><a href=\"https://www.usenix.org/sites/default/files/conference/protected-files/srecon16europe_slides_rabenstein.pdf\">Alerting for Distributed Systems</a></li>\n<li><a href=\"https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit\">Philosophy on Alerting</a></li>\n</ul>\n<h2 id=\"style-guide\" style=\"position:relative;\"><a href=\"#style-guide\" aria-label=\"style guide permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Style Guide</h2>\n<p>The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in <a href=\"https://datatracker.ietf.org/doc/html/rfc2119\">RFC 2119</a>.</p>\n<ul class=\"pf-c-list\">\n<li>Alert names MUST be CamelCase, e.g.: <code class=\"language-text\">BackendServiceStuck</code></li>\n<li>\n<p>Alert names SHOULD be prefixed with a component, e.g.: <code class=\"language-text\">ZookeeperPersistentVolumeFillingUp</code>, or a Service Level Objective (SLO) name if the alert is a <a href=\"https://sre.google/workbook/alerting-on-slos/#6-multiwindow-multi-burn-rate-alerts\">Multi-Window, Multi-Burn Rate alert</a></p>\n<ul class=\"pf-c-list\">\n<li>There may be exceptions for some broadly scoped alerts, e.g.: <code class=\"language-text\">TargetDown</code></li>\n</ul>\n</li>\n<li>\n<p>Alerts MUST include a <code class=\"language-text\">severity</code> label indicating the alert’s urgency.</p>\n<ul class=\"pf-c-list\">\n<li>Valid severities are: <code class=\"language-text\">critical</code> or <code class=\"language-text\">warning</code> — see below for\nguidelines on writing alerts of each severity.</li>\n</ul>\n</li>\n<li>\n<p>Alerts MUST include <code class=\"language-text\">summary</code> and <code class=\"language-text\">description</code> annotations.</p>\n<ul class=\"pf-c-list\">\n<li>Think of <code class=\"language-text\">summary</code> as the first line of a commit message, or an email\nsubject line.  It should be brief but informative.  The <code class=\"language-text\">description</code> is the\nlonger, more detailed explanation of the alert.</li>\n</ul>\n</li>\n<li>\n<p>Where applicable, alerts SHOULD include a <code class=\"language-text\">namespace</code> label indicating the source of the alert.</p>\n<ul class=\"pf-c-list\">\n<li>Many alerts will include this by virtue of the fact that their PromQL\nexpressions result in a namespace label.  Others may require a static\nnamespace label.</li>\n</ul>\n</li>\n<li>\n<p>Alerts SHOULD include a cluster identifier label, especially in cases where alerts are defined or routed centrally for multiple services &#x26; clusters. This cluster identifier label SHOULD be consistent and obvious - for example, <code class=\"language-text\">cluster</code>.</p>\n<ul class=\"pf-c-list\">\n<li>This can hint to an SRE where the affected service is located. It can also be useful for aggregation &#x26; filtering when analysing groups of firing alerts.</li>\n</ul>\n</li>\n<li>All alerts MUST include an annotation (e.g. <code class=\"language-text\">runbook_url</code>) which directs the responder(s) to the location of the runbook (sometimes called a Standard Operating Procedure or SOP) that is specific to fixing that problem.</li>\n</ul>\n<h2 id=\"critical-alerts\" style=\"position:relative;\"><a href=\"#critical-alerts\" aria-label=\"critical alerts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Critical Alerts</h2>\n<p>Critical alerts are for alerting current and impending disaster situations.  These alerts page an on-call Engineer.  The situation should warrant waking someone in the middle of the night and/or interrupting their regular workflow.</p>\n<p>Reserve critical level alerts only for reporting conditions that may lead to service unavailability.\nFailures of individual components MUST NOT trigger critical level alerts unless that failure is likely to affect a related SLO if not attended to.\nConfigure critical level alerts so they fire before the situation becomes irrecoverable.</p>\n<p>Example critical alert:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token punctuation\">-</span> <span class=\"token key atrule\">alert</span><span class=\"token punctuation\">:</span> BackendServiceStuck\n  <span class=\"token key atrule\">expr</span><span class=\"token punctuation\">:</span> max_over_time(backend_state<span class=\"token punctuation\">[</span>5m<span class=\"token punctuation\">]</span>) <span class=\"token tag\">!=</span> 1\n  <span class=\"token key atrule\">for</span><span class=\"token punctuation\">:</span> 10m\n  <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">severity</span><span class=\"token punctuation\">:</span> critical\n  <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">summary</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'The backend service is stuck in a non-ready state'</span>\n    <span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'The backend service {{ $labels.name }} in the {{ $labels.namespace }} namespace, managed by operator {{ $labels.pod }} has been in a non-ready state for 10 minutes'</span>\n    <span class=\"token key atrule\">runbook_url</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'https://example.com/backend_service_stuck.asciidoc'</span></code></pre></div>\n<p>This alert fires if a backend service has <em>not</em> been ready for a pre-configured time. To avoid false alarms the chosen timeout MUST be long enough that it is unlikely to be reached during any routine restart or deployment.\nThis is a clear example of a critical issue that represents a threat to the operability of the service, and likely warrants paging someone.\nThe alert has a clear summary and description annotations, and it links to a runbook with information on investigating and resolving the issue.\nIf there is an SLO related to the alert, mention it in the description so the responder knows why the alert is important.</p>\n<p>The group of critical alerts should be small, very well-defined, highly documented, polished and with a high bar set for entry.</p>\n<h2 id=\"warning-alerts\" style=\"position:relative;\"><a href=\"#warning-alerts\" aria-label=\"warning alerts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Warning Alerts</h2>\n<p>The vast majority of alerts should use this severity.\nIssues at the warning level should be addressed in a timely manner, but don’t pose an immediate threat to the operation of the service as a whole.\nIf your alert does not meet the criteria in “Critical Alerts” above, it belongs to the warning level.</p>\n<p>Use warning level alerts for reporting conditions that may lead to inability to deliver individual features of the service, but not the service as a whole.\nMost alerts are likely to be warnings.\nConfigure warning level alerts so that they do not fire until components have sufficient time to try to recover from the interruption automatically.\nYou should have a policy for triaging warning alerts and responding in a timely manner e.g. issues created automatically on a backlog that is being actively worked from.</p>\n<p>Example warning alert:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token punctuation\">-</span> <span class=\"token key atrule\">alert</span><span class=\"token punctuation\">:</span> PrometheusDuplicateTimestamps\n  <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> Prometheus <span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span>$labels.namespace<span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>/<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span>$labels.pod<span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span> is dropping\n      <span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> printf \"%.4g\" $value  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span> samples/s with different values but duplicated\n      timestamp.\n    <span class=\"token key atrule\">summary</span><span class=\"token punctuation\">:</span> Prometheus is dropping samples with duplicate timestamps.\n    <span class=\"token key atrule\">runbook_url</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'https://example.com/prometheus_duplication_timestamps.asciidoc'</span>\n  <span class=\"token key atrule\">expr</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n    rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0</span>\n  <span class=\"token key atrule\">for</span><span class=\"token punctuation\">:</span> 1h\n  <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">severity</span><span class=\"token punctuation\">:</span> warning</code></pre></div>\n<p>This alert fires if prometheus is dropping samples that have different values but the same timestamp.\nAlthough not ideal, it’s also not a current or impending disaster situation and hence shouldn’t need to wake someone in the middle of the night.\nThe 1h timeline allows sufficient time for the service to resolve the problem itself before firing.\nIf the alert does fire, it can be triaged during normal working hours.\nLike with critical alerts, if there is an SLO related to the warning alert, mention it in the description so the responder knows why the alert is important.</p>","fields":{"srcLink":"https://github.com/operate-first/SRE/blob/master/prometheus_alerting_consistency.md"},"frontmatter":{"title":"","description":null,"extraClasses":null,"banner":null}}},"pageContext":{"id":"9f2d9ca9-ce65-5a54-9655-b03d5520f68b"}},"staticQueryHashes":["1764348645","2823140819","3000541721","3606484676"]}