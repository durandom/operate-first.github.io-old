{"componentChunkName":"component---src-templates-markdown-remark-js","path":"/data-science/ai4ci/docs/workshop/model_development.md","result":{"data":{"site":{"siteMetadata":{"title":"Operate First"}},"markdownRemark":{"id":"16a2de78-8abd-576c-827c-5f63d0fb24d3","html":"<h1 id=\"model-development\" style=\"position:relative;\"><a href=\"#model-development\" aria-label=\"model development permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Development</h1>\n<h2 id=\"set-environment-variables\" style=\"position:relative;\"><a href=\"#set-environment-variables\" aria-label=\"set environment variables permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Set Environment Variables</h2>\n<p>This is an important step to get right for successfully being able to run the notebooks.</p>\n<ul class=\"pf-c-list\">\n<li>Go to <a href=\"https://jupyterhub-aiops-tools-workshop.apps.smaug.na.operate-first.cloud/hub/spawn\">Jupyterhub</a> and select the ocp ci analysis image and the “Workshop” container size</li>\n<li>Open the terminal and clone your forked Github repository in Jupyterhub.</li>\n</ul>\n<p><code class=\"language-text\">git clone https://github.com/{YOUR-USERNAME}/ocp-ci-analysis</code></p>\n<ul class=\"pf-c-list\">\n<li>Set up env variables at the root of the repository.</li>\n</ul>\n<p><code class=\"language-text\">cd ocp-ci-analysis</code></p>\n<p><code class=\"language-text\">vi .env</code></p>\n<p>Add the following contents to the .env file. For Bucket and Trino credentials refer to this link.</p>\n<p>Save the file and start running the notebooks.</p>\n<p><strong>Note</strong>: We would be downloading Pull Requests data from a Github repository.</p>\n<p>For faster download, choose a repository which does not have too many PRs. For example purposes, we have chosen a repository with ~1000 PRs</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">    GITHUB_REPO= Github repository that you wish to download\n    GITHUB_ORG= Github organization that the repository belongs to. If it&#39;s on your personal account, this will be your username.\n    S3_ACCESS_KEY= S3 bucket access key\n    S3_ENDPOINT_URL= S3 bucket endpoint\n    S3_BUCKET= S3 bucket name\n    S3_SECRET_KEY= S3 bucket secret key\n    CEPH_BUCKET= S3 bucket name\n    CEPH_BUCKET_PREFIX= set this to your username, this is the location/key where the files will be stored on S3 storage\n    CEPH_KEY_ID= S3 bucket access key ID\n    CEPH_SECRET_KEY= S3 bucket secret key\n    GITHUB_ACCESS_TOKEN_TODAY= Your Github personal access token generated from the previous step\n    TRINO_USER= trino user\n    TRINO_PASSWD= trino password\n    TRINO_HOST= trino host\n    TRINO_PORT= trino port\n    CHOSEN_MODEL= Model that you wish to choose for deployment. Either one of &#39;rf&#39;, &#39;xgbc&#39;, &#39;svc&#39;, &#39;gnb&#39;</code></pre></div>\n<h2 id=\"model-development-1\" style=\"position:relative;\"><a href=\"#model-development-1\" aria-label=\"model development 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Development</h2>\n<p>In this workshop, we will be training a machine learning model using tools and services available on Operate First.</p>\n<p>The aim of this model is to take a Github repository of interest and predict the time that it will take to merge a new Pull Request. For that purpose, we frame the “time taken to merge a PR” as a classification problem where we predict whether the time taken to merge a PR falls within one of a few predefined time ranges.</p>\n<p>Browse through the following notebooks to understand the model development process,    to directly see the model development running in a pipeline or in action, skip to the next section</p>\n<h3 id=\"data-collection\" style=\"position:relative;\"><a href=\"#data-collection\" aria-label=\"data collection permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data collection</h3>\n<p>In order to collect the data from Github repositories, we use the thoth-station <a href=\"https://github.com/thoth-station/mi-scheduler\">MI- Scheduler</a> that collects and analyzes metadata information from Github repositories and stores them on ceph object storage. We use the MI-Scheduler tool to collect Pull Request data from a repository of your choice.</p>\n<h3 id=\"feature-engineering\" style=\"position:relative;\"><a href=\"#feature-engineering\" aria-label=\"feature engineering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature engineering</h3>\n<p>After collecting the data, we perform some initial exploration such as correlation analysis on the dataset to discover any interesting insights. We then engineer features which are needed to train a classification model which predicts the time to merge of a PR.</p>\n<p>We transform the input columns obtained from pull requests such as size of a PR, types of files added in a PR, description of a PR into various features which can be ingested by an ML Model.</p>\n<h3 id=\"model-training\" style=\"position:relative;\"><a href=\"#model-training\" aria-label=\"model training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model training</h3>\n<p>After performing initial data analysis and feature engineering, we train a machine learning model to classify the time<em>to</em>merge values for PRs into one of 10 bins (or “classes”). To train this model, we use the features engineered from the raw PR data. We explored various vanilla classifiers, like Naive Bayes, SVM, Random Forests, and XGBoost and save the best performing model on S3 storage.</p>","fields":{"srcLink":"https://github.com/aicoe-aiops/ocp-ci-analysis/blob/master/docs/workshop/model_development.md"},"frontmatter":{"title":"","description":null,"extraClasses":null,"banner":null}}},"pageContext":{"id":"16a2de78-8abd-576c-827c-5f63d0fb24d3"}},"staticQueryHashes":["1764348645","2823140819","3000541721","3606484676"]}