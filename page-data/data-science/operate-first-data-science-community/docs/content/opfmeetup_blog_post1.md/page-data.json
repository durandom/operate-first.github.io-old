{"componentChunkName":"component---src-templates-markdown-remark-js","path":"/data-science/operate-first-data-science-community/docs/content/opfmeetup_blog_post1.md","result":{"data":{"site":{"siteMetadata":{"title":"Operate First"}},"markdownRemark":{"id":"56f90f31-8a4c-53ea-96e5-9a823559d0c9","html":"<p><em>This blog was generated by AI</em></p>\n<h2 id=\"operate-first-for-data-science\" style=\"position:relative;\"><a href=\"#operate-first-for-data-science\" aria-label=\"operate first for data science permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Operate First for Data Science</h2>\n<h3 id=\"date-2021-11-16\" style=\"position:relative;\"><a href=\"#date-2021-11-16\" aria-label=\"date 2021 11 16 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Date: 2021-11-16</h3>\n<h3 id=\"speaker-michael-clifford\" style=\"position:relative;\"><a href=\"#speaker-michael-clifford\" aria-label=\"speaker michael clifford permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Speaker: Michael Clifford</h3>\n<p>Speaker1    00:00:02    Cool. So the meeting is being recorded. And then I’m gonna go ahead and start sharing my screen and get started. All right. Great. Is everyone seeing a bright red slide?</p>\n<p>Speaker0    00:00:27    Yes. Okay, cool. </p>\n<p>Speaker1    00:00:30    Welcome everyone right to our inaugural Operate first data science community meetup. Thank you all for for attending. My name is Michael Clifford and I’m a data scientist and data science manager working at Red Hat in the office of the CTO as a member of the AI center of excellence, where I’m working on among other things, the operate first project. So today is really gonna be like episode one, right? Like what is operate first for data science. And today is gonna be about introducing the concepts of operate first to everyone, why it’s relevant to data scientists and how you can, get started today, getting on the operate first platform and spinning up a development environment of your own. So hopefully this shouldn’t take like the full hour. So any time leftover can be used for discussion or Q&#x26;A or whatever we wanna do with the rest of that time. Cool. So let’s get started. So operate first is like a, it’s a large project that really goes beyond strictly data science related topics and a full overview of what operate first is and a little bit outside of the scope of what I wanna talk about today. But I  encourage all of you to go to operate first (operate-first.com) after the talk and check out the kind of project overall. Today I’m going to focus on kind of outlining enough of the project to make sense of the operate first data science community. And my hope is that the next 15 minutes, right, the relationship between operate first at large and the operate first data science community will become really clear. So cool. So our goal for this operate first data science community is really about, we want to educate and collaborate with anyone who’s interested in learning more about how people are working towards democratizing the development of production level AI, and specifically with our regard to doing that in the domain of cloud native development. And really what that means, right, is it’s about like kind of collectively investigating the question, how do we make Cloudnative data science development, more open, more accessible, and ultimately more reliable? And my hope is that we can do this together by addressing the kind of multidisciplinary nature of the current state of AI development and cloud operations, and kind of covering technical topics together that are relevant to data scientists, to software engineers, to DevOps professionals, and really like the intersection of the work of those groups. And again, to do this all like in a fully public forum, like we’re doing today. Great. So, I hope that was clear of the goals of what we’re trying to do with this community. Next I will quickly provide an overview of the operate first project, that should put into context how and why data science is specifically important to this work. Sorry, There’s an animation here that I forgot about, ‘the operate first data science community’. Great. So, right in short, the operate operate first is kind of both an idea and a project. It’s an approach. It’s a way of approaching cloud operations and software development, but it’s also like a group of real people and real machines that are actually implementing this approach. So, this is a definition that creators of operate first came up with for what it means. I’m not gonna read it here verbatim, but you can read it if you’d like. But really what it means is it’s about addressing, an existing gap in the software development life cycle, where like the actual operating of an application is not sufficiently proved out by the developers and the knowledge of how to operate these applications. The operational experience does not exist in any kind of traditional open source format. So by providing an open production community cloud where we operate our software first, Operate first, right. We are able to extend the open source software development process to include operating in the cloud essentially. Cool. So that’s like the definition, the idea behind operate first, the goal of operate first. But why do we think that it would actually work? Like why will it work? So I am gonna read this statement again from the, the inceptors of the operate first idea. They think that it should work because having developers and also that extends to us data scientists bringing their code to a production cloud where they can work with the cloud operators and experience firsthand its operational considerations as well as betting the functionality of the platform can create this feedback loop that allows developers to address and enhance their codes operations, and ideally fixed bugs as well. So I think that that’s fairly clear self explanatory, right? By running projects on a community cloud where operators and developers are directly interacting with one another, this generates this beneficial feedback loop that can improve both the applications themselves and the platform. Cool. So that’s why we think that the operate first should work as a, as a development process and a open source project. But so far, this has been a little bit of talk around updated philosophy around software development. You might be asking yourself like, okay, so like, is any of this real, like where is this community cloud that you’re referring to? And the answer to that question is yes. So we do currently have an existing community hybrid cloud set up. We currently have clusters in the United States that are associated with Boston university, part of the MOC, which is the Massachusetts open cloud. We have some clusters in Germany as well as one that’s part of an AWS cluster and we’re continuing to work on extending this community cloud. And all of this is managed through our operate first organization that is publicly a available on GitHub. So what are we doing with these clusters? what are we operating? So we run Kubernetes and OpenShift on top of them. We are using GitOps best practices in public repos to manage everything that’s being done there. We are generating operational data that we can make public as well. Um, and I’ll talk a little bit later about why that’s really important and relevant to our data science folks. And then finally on top of all of that, we are running real workloads, applications. And the main workload that we are currently operating first at the moment is Open Data Hub or ODH. So that just like quickly to let all of you know, that this community cloud is a real thing and it currently exists and we are running workloads on top of it today. So now I wanna talk a little bit about the specific workload that we’re actually running right now. So it’s called open data hub. What is open data hub? It is what we refer to as like a meta project. So it integrates some open source tools aimed at providing an end-to-end machine learning application on top of OpenShift. A meta project is really aimed at integrating multiple open source projects into one project that is easily deployed by users. Open data hub also has like both upstream and downstream relationship with many of the open source projects that it’s involved with. The main downstream project is kubeflow. However, it’s also downstream of other projects like Seldon, Elyra, Kafka, Spark, Grafana and Prometheus. And so all of these things taken together provide this comprehensive end to end machine learning platform that we call open data hub. So when we say we’re running data hub, it really means that we’re running all of these services for data scientists. So Open Data Hub is like a huge separate project in its own right. And if you’re interested in getting more involved or learning more about the open data hub side of things, I encourage you to go to open data hub.io and check it out. But if you just wanna get started, messing around with it, you can start using it right now through operate first. Now hopefully I’ve put some context around what is operate first and what is currently being operated there. And I hope all of you are starting to see how data science is relevant to this project. And specifically, the main tool that we are operating is one that’s designed specifically for data scientists. But I think what we’re doing here really goes a bit beyond just being users of the operate first cluster and open data hub. So where is the data science, right. So, now you might be asking yourself, like, why should I get involved with the operate first data science community? What’s in it for me? Well, as a data scientist, myself, there are a couple of things that are important to me that I would look for in the data science community. And that first thing is resources. So having access to complete resources to do my work, particularly those that are free of any operational costs to me. Solid ways of collaborating with other data scientists to share notebooks and other reproducible content and experiments and just general data science, software engineering, collaboration, a way to create reliable and reproducible experiments and environments. An easy path for me to actually get my models out into the world with simple deployments of pipelines and inference models. And then last but not least like interesting problems and interesting data to actually get access to. So if I’m a data scientist, these are the kinds of things that are important to me and obviously there’s not like a comprehensive list. I think it does a pretty good job of covering some of these major items. So, does getting involved with the operate first data science community address any of these requests? Well, I think it gives me, an OpenShift cluster with open data hub deployed to develop and run my machine learning applications. It also provides access to the operators and developers. So the operators of the cloud and the developers of the tool, like open data hub, where we can actually have an impact on how those tools are getting developed moving forward. The community already has a number of GitHub organizations and repositories along with some defined data science best practices that are being constantly refine that we use to share and collaborate openly on data science projects. We have tools in place. By using GIT to help us build custom images so that we can publish and share reproducible experiments and other content that we want to share with folks. The images that we are using through Jupyterhub currently include things like Elyra, Kubeflow and Seldon for simple machine learning deployment and model serving. In addition to that, we also have a solid user base like myself and other members of our team, and documentation of using these tools. So it’s really easy to get good support and feedback. The final thing is that since there is a community cloud, it’s generating real production operations data that can be made publicly available for tackling machine learning problems in the AIOps domain. I’ll get into a second why that’s that’s important. As you can see from this list, there’s a lot of stuff provided. As a cloud native data scientist, there is a lot of things here, that sound good to me. That’s resources, services, support access to the people, maintaining, developing, and using all of these things, as well as access to interesting data. So, I hope that’s kind of enough reason to stick around and get involved with the community here. There’s one more thing that I wanted to discuss, and that’s kind of how AIops fits into this picture. So I’m not gonna give a full discussion here on what AIOps is. But maybe we’ll do an upcoming talk about that. But in short, it’s the idea that we can improve IT operations through the application of some AI tools. And unfortunately, like AIOps currently suffers from a data problem. And so in order to do machine learning, artificial intelligence, we need to have some data. However, the data that’s being generated that would be useful for AIOps is like largely internal server data. And it’s not something that companies generally feel comfortable open sourcing and making publicly available. So this means that there’s no endness of AI ops right now. There’s no real trusted benchmark data set that can be used to reliably push the field forward. So one of the additional goals of the operate first data science communities is to kind of generate and collect this data. And this is actually done like by us, by just using the platform. So as data scientists, we are generating not just baseline operations data, but like machine learning, workload operations data. So we can generate data that describes what happens to a machine when a cluster is running, what some are considering to be like the most important workload of the 21st century, like machine learning applications. So with that, we can now start to work on developing open source applications that actually target the operations of the cluster that we run our very own machine learning workloads on. So that’s pretty cool. That would then feed back into the operations platform, of course. So since we’re already closely integrated with the platform operators, we can get our solutions implemented and tested much rather easily. This really creates that cycle like we discussed at the beginning. Another beneficial feedback loop that can improve both our applications as well as the, the platform itself. So I just wanted to make that clear, that the operate first data science community is both about using the platform for data agnostic projects and exploring new cloud native data science tooling, as much as it’s also about improving that very same platform itself through explorations of AIOps projects. I know there was a lot of talk. Does anyone have any questions at this point? The next thing I’m gonna do is give a quick demo for how to actually get onto the cluster and spin up a Jupyterlab environment and start to work there. Before I do that, any questions? Awesome. So, and then for this demo. This is like an open community cloud, so I encourage you all to follow along. Hopefully it should work fine for you. So the first thing to do here is to go to operate-first.cloud. This is like the main hub for the operate first project. Um, as I said, there’s much more than just what we’re talking about today and encourage you to investigate this website more fully to get a bigger sense of the project. But if you just wanna get started with some data science stuff, you can can go to the hitchhikers guide here. And on the left hand side here, you’ll see a number of applications that are actually being operated. The one we want to look at here is open data hub and go to overview. And this will show us the list of components that are currently being deployed. The cluster names change as the clusters evolve, but we’re currently working with smaug. So we can go to Jupyterhub opf, Jupyterhub apps, smaug. Cool. So first thing you have to do is log into the cluster. I believe if you have a GitHub account, you can just authenticate through GitHub. So we can do that, click the operate first. And then it will take you to a Jupyterhub spawner page. I’m assuming also here that mostly everyone is familiar with Jupyter notebooks and Jupyter lab. We’ll have another talk in a few weeks, then we will get more into the details of how they work. So this is Jupyter hub, which is essentially like a, Kubernetes OpenShift application for deploying Jupyterhub pods for us to work on our clusters. And the first thing that you get here is a spawner page where you have some options about what it is that is going to be spawn for you. The first thing that you’ll notice is these notebook images, and there’s a whole list of them. So these are maintained by our teams and you can go ahead and create your own and add it to this list. So this is the way that we publish and share reproducible content. So there’s two types of images that you’ll see here. And again, how we generate these will also be in a future talk, there’s also docs and videos and resources that currently exist that have some of this stuff. But you can see here something like, AICoE Elyra AIDevSecOps tutorial. So this is an image that contains both the requirements for running the code that’s there, as well as the content, as well as the notebooks and different things that are needed. So somebody’s work is being packaged and represented in this image. Then we have other ones like the Ray ML notebook or Elyra notebook that are just kind of working environments that have some predefined set of requirements that we’re gonna use. There’s no content associated with it. It’s just a stock of requirements. So I’m gonna pick the Elyra notebook image today because it’s a clean, nice one that has Jupyterlab. Then, the next thing that we have here is the deployment size. So we can actually choose what size of container we want to deploy. There’s resources that can be set by the administrator for your particular account, that would be defaults. But you also have the option of choosing small, medium, or large, depending on the specific work that you’re going to do that day. So smalls, maybe just reading some notebooks, large as if you’re gonna actually be doing some data crunching or whatever that might be. So today I’ll just do small. And then it also allows us to set our environment variables ahead of time, before we spawn our pod. So this way we can inject specific information into our pod that we probably don’t want exposed. So information like API credentials and things like that, but it could be anything. So you can add more variables, all this S3_KEY, I’ll call it 1234 and make it a secret, so nobody else can see it. You can go on and do this as much as you want to have all of the environment variables that you need injected into your pod. One of the main reasons we do this is to specifically put S3 credentials in, because even though you have a persistent volume claim, where you can keep some information. It’s better used for, like notebooks and Python code and not really big data sets. For that we rely on interacting with an S3 bucket. Um, so cool. So let’s go ahead and start our server. Uh, so this might take a couple minutes.</p>\n<p>Speaker4    00:25:07    Michael, any question at any time, right?</p>\n<p>Speaker1    00:25:12    Yeah, sure.</p>\n<p>Speaker4    00:25:13    Good. Well, just a reminder, that’s my question.</p>\n<p>Speaker1    00:25:16    Yes. Questions anytime. This also might be a good time for questions, but maybe it’s going quicker than I thought. Anyways, what this is doing now is essentially spinning up a pod, an ephemeral pod and connecting it to the persistent volume claim that you have. So that’s kind of like your local storage in the cloud. So, like I said, as you do work, you work on a particular GitHub repo or some project or anything in there. Those things will stay. But any changes you actually make to the working environment will be reset every time you start a new pod. So that’s just something to be aware of. Any questions?  </p>\n<p>Speaker5    00:26:12    I’ve got a question Michael. Are there any GPU’s available in these clusters?</p>\n<p>Speaker1    00:26:16    So at the moment, there are no GPUs available. Well, there’s one in the AWS cluster, there’s a couple GPUs available, which is associated with the OS climate project. So we have small cluster, we know that the GPU are physically at the MOC and we’re having them, we’re waiting to get them like installed.</p>\n<p>Speaker0    00:26:43    Cool.</p>\n<p>Speaker1    00:27:29    Sorry. I’m not sure what’s going on here. Oh, here we go. Okay, Great. So this is what the Jupyterlab environment looks like. So, as a data scientist, it’s like one of the things we’re thinking about here is how do we migrate the work of a data scientist off of their local machine, into a cloud native environment? And so this is like the Jupyterlab is something I would use on my machine. So it looks identical to what it would be locally used for me. So I hope that’s clear. I’m not sure exactly how many people here are familiar with the Jupyterlab, jupyter notebook environments, but, I’m just gonna assume some familiarity and show you what we can do here. So with Jupyter lab, we’ve now spawn up a pod for us to work in. You can see here on the left hand side is the, a directory of my PVC where my like, existing work has been maintained. I can do a number of things when I get on. I can spin or I can generate a new Python notebook. I can create a console, Elyra tooling that, that we’ll talk about again in a later talk. And I could also just open up a terminal. So it really exists as my workspace and from the terminal, I can do all kinds of normal things, as long as it doesn’t require sudo. I can pretty much do anything here. So I can see all of the work, all the directories that I have available to me. Maybe I’m interested in how much space I actually have left on my PVC because for whatever reason, if the PVCs get full, I can cause some weird issues. So I wanna make sure that’s all good before I maybe download a data set that I’m gonna mess with. And so I can just do df -h and that will show me that I have 17 gigabytes of free space. That’s cool. I can also check what Python packages I have available to me. Just all kinds of normal command line stuff that you would want to do.  That makes it pretty easy to do things that you’d want to do that you can’t necessarily do directly inside of a notebook. But beyond that, you can open a notebook quickly. So the first thing I want to show you all is just that those environment variables that we set earlier actually made it into the pod. So I’m importing OS and hitting shift-enter to run the cell and create a new one below. You call it S3_KEY. And it should tell us that we have the right environment variable. Oh, did not work. Should I call it something different? Yes. Great.So these environment variable that we had sent set in this spawner page is available to us and accessible. We can also now like do whatever we would normally do in a Jupyter notebook, that is writing Python code. We can also create markdown in order to annotate the code that we’re working on, we can generate graphs, we can do all kinds of different things. We can also rename this. And actually get out of it and be done working. One of the things to note is that these kernels and notebooks will still run in the background. So you can actually go to this little tab here and shut everything down. So showing down the kernel, as well as showing down the terminals to free up space. So yeah, this is the Jupyterlab environment. I prefer it a little bit over the Jupyter notebook environment. It’s just a little bit more of like a modern IDE. It also has a more extensive suite of extensions, including extension manager. So you can get some additional tooling in there. So, yeah, that’s mainly what I wanted to show everyone today. Hopefully people were able to follow along and actually do some of this stuff and get started. The last thing to be aware of is this is obviously like a shared environment. So when you’re done working, it’s good practice to go to the hub control panel and stop your server. And that should free up the resources for everyone else. When you’re working on this stuff, you might be asking yourself, okay. I was like, where is the community part of any of that? So when you’re working on that, I come to operate first cloud website, but more importantly, up here, our number of important links. So there’s our GitHub repository, sorry, GitHub organization for operate first. And here we have specifically the support organization and here is not really used to manage any code. It’s more about creating issues and having conversations about help and support and different things you need for managing any of the operate first clusters that we’re working on. So that’s one way to get support, get engaged with, the other more immediate, but often still ties back to the GitHub, is to get involved with slack. So if you join our slack community, there is a support page where we’re discussing things all the time. You can ask why is my POD not working or anything of interest to you. There’s also a data science channel where we can just, oh, you are not seeing any of this. Sorry. Well, trust me, if you go on to the link, there’s a bunch of channels where you can get support and talk to people. If you are interested in just like learning more about stuff and maybe jumping ahead of some of the content that we plan to give over the next few meetups. You can go to our YouTube channel, Operate first and learn a bunch of stuff. The two that are probably most interesting to people here would be the espresso series and then the AI for continuous integration, where we actually are doing a real project, implementing a lot of these things in the AI ops domain. That was everything I wanted to share with everyone today. So, there were some questions that came through here.</p>\n<p>Speaker4    00:36:07    Just a comment. So Daniel asked for the slack link. I think it’s on the upper right corner of the operate first website, right? There’s an invite.</p>\n<p>Speaker1    00:36:17    Yeah. So if you click on here, you should be able to get the invites to slack.</p>\n<p>Speaker4    00:36:29    I’ll stop it Michael, upper right corner. That was an invite link. That’s good enough.</p>\n<p>Speaker1    00:36:32    So any other questions, concerns, discussion?</p>\n<p>Speaker4    00:36:53   Another question. So, who are you guys, that were the question to, is GPU available? There were the question, how to, how to put these environment variables in there. So, we are trying to cover a lot of personas, right? So, we are focusing on the data scientists, but also on the, open data hub operator himself, herself. So maybe just a short ping in the chat. Are you a data scientist or more like a data scientist? I’m pretty sure not everybody of us is just single person. We are most often split brain. So maybe you can just put a short data scientist or ODH operator or openshift operator or an application developer thing in the chat so that we know who you guys are. I mean, a few names, I don’t know. So that’s okay. You’re free.</p>\n<p>Speaker1    00:37:56    You could put it in this chat. You could also put it in the slack channel, so it doesn’t evaporate after this meeting.</p>\n<p>Speaker4    00:38:04    I’ll just talk about it and introduce yourself.</p>\n<p>Speaker1    00:38:18    Any other question, discussion points, anyone else? So, actually one other thing I’d like to share with everyone before we, get off here is that again, this is a community. We are kind of have some idea of what we want to share with everyone over the next couple of sessions. But obviously if you have something that you think would be interesting or relevant or get on to the cluster and start working on a project. That’s something we’d very much like to hear about. And the main way in which we are trying to keep track of all of that work is through an open GitHub repository. And you can, or I would appreciate, if you opened an issue, in the operate first data science community repository, we can keep track, of topics that way. That was the last little piece of housekeeping I wanted to share with everyone. Great. If there’s nothing else, we can get off a few minutes early, but I’ll stay a little bit longer if anybody wants to chat about this or anything else. </p>","fields":{"srcLink":"https://github.com/aicoe-aiops/operate-first-data-science-community/blob/master/docs/content/opfmeetup_blog_post1.md"},"frontmatter":{"title":"","description":null,"extraClasses":null,"banner":null}}},"pageContext":{"id":"56f90f31-8a4c-53ea-96e5-9a823559d0c9"}},"staticQueryHashes":["1764348645","2823140819","3000541721","3606484676"]}